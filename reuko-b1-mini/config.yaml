# Model Yapılandırması
model_config:
  n_positions: 1024
  n_ctx: 1024
  n_embd: 768      # GPT-2 small için 768, medium için 1024
  n_layer: 12       # GPT-2 small için 12, medium için 24
  n_head: 12        # GPT-2 small için 12, medium için 16

# Veri Seti Yapılandırması
dataset_config:
  name: "wikitext"
  # subset: "wikitext-103-raw-v1" # Daha büyük veri seti için bunu kullanın
  subset: "wikitext-2-raw-v1" # Test için daha küçük veri seti
  max_length: 1024

# Eğitim Argümanları
training_args:
  output_dir: "./results/reuko-b1-mini"
  num_train_epochs: 3
  per_device_train_batch_size: 2      # Daha düşük GPU belleği kullanımı için
  per_device_eval_batch_size: 4       # GPU belleğinize göre ayarlayın
  gradient_accumulation_steps: 8      # Etkili batch boyutunu korumak için artırıldı
  learning_rate: 0.00005              # Standart başlangıç değeri
  weight_decay: 0.01                  # Aşırı öğrenmeyi önlemek için
  warmup_steps: 500                   # Öğrenme oranını yavaşça artırmak için
  fp16: true                          # Karışık hassasiyetli eğitim (NVIDIA GPU'lar için)
  logging_dir: './logs'
  logging_steps: 50
  evaluation_strategy: "steps"
  eval_steps: 1000
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 3                 # En fazla 3 checkpoint sakla
  report_to: "tensorboard"            # veya "wandb"

# DeepSpeed Yapılandırması (isteğe bağlı)
# deepspeed_config: "deepspeed_config.json"
