# Core ML Libraries
torch>=2.1.0
transformers>=4.35.0
datasets>=2.14.0
tokenizers>=0.14.0
sentencepiece>=0.1.99
accelerate>=0.24.0
evaluate>=0.4.0

# Data Processing
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0

# Development
jupyter>=1.0.0
notebook>=7.0.0
ipywidgets>=8.0.0

# Configuration & Utilities
pyyaml>=6.0.1
tqdm>=4.66.0
psutil>=5.9.0
GPUtil>=1.4.0

# Modern Training Optimizations
deepspeed>=0.11.0  # ZeRO optimization for 1B+ models
bitsandbytes>=0.41.0  # 8-bit & QLoRA training
peft>=0.5.0  # Parameter-Efficient Fine-Tuning (LoRA, AdaLoRA)
flash-attn>=2.3.0  # Flash Attention for memory efficiency

# Monitoring & Logging
wandb>=0.16.0  # Weights & Biases for experiment tracking
tensorboard>=2.14.0
matplotlib>=3.7.0
seaborn>=0.12.0

# Text Processing & Evaluation
nltk>=3.8.1
rouge-score>=0.1.2
sacrebleu>=2.3.1
bert-score>=0.3.13

# Multi-GPU & Distributed Training
fairscale>=0.4.13  # Alternative to DeepSpeed
torch-distributed>=0.1.0

# Model Serving & Deployment
fastapi>=0.104.0
uvicorn>=0.24.0
gradio>=3.50.0  # Quick model demos

# Development Tools
black>=23.9.0
isort>=5.12.0
flake8>=6.0.0
pytest>=7.4.0
mypy>=1.6.0

# Optional: TPU Support
# torch-xla>=2.1.0  # Uncomment for TPU training
