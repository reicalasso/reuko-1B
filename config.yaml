# Reuko-1B Configuration - Modern 1B Parameter Model

model:
  # Model Selection (pick one):
  name: "t5-large"  # 770M params - closest to 1B
  # name: "google/flan-t5-large"  # 780M params - instruction tuned
  # name: "microsoft/DialoGPT-large"  # Alternative for conversational tasks
  
  # Architecture
  use_custom_1b: true  # Use our custom 1B architecture
  max_input_length: 1024  # Modern context length
  max_output_length: 256  # Detailed outputs
  
  # Generation Parameters
  num_beams: 6
  early_stopping: true
  do_sample: true
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.2
  length_penalty: 1.0

# Modern Training Configuration
training:
  # Batch Configuration for 1B Model
  batch_size: 1  # Per device batch size
  eval_batch_size: 1
  gradient_accumulation_steps: 32  # Effective batch = 32
  max_grad_norm: 1.0  # Gradient clipping
  
  # Training Schedule
  num_epochs: 3
  max_steps: -1  # Set to specific number if needed
  
  # Optimizer & Learning Rate
  optimizer: "adamw"  # Modern default
  learning_rate: 1e-4  # Conservative for large models
  weight_decay: 0.1  # Strong regularization
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  
  # Learning Rate Schedule
  lr_scheduler_type: "cosine"  # Modern choice
  warmup_ratio: 0.1  # 10% of training for warmup
  warmup_steps: 1000  # Alternative to ratio
  
  # Memory Optimizations
  fp16: false  # Disable for stability
  bf16: true   # Better than FP16 for modern hardware
  fp16_opt_level: "O1"
  dataloader_pin_memory: true
  dataloader_num_workers: 4
  
  # Checkpointing
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 3
  eval_strategy: "steps"
  eval_steps: 500
  logging_steps: 100
  
  # Advanced Optimizations
  gradient_checkpointing: true  # Critical for 1B models
  deepspeed_stage: 2  # ZeRO stage 2 optimization
  use_8bit_optimizer: true  # bitsandbytes optimization

# Multi-task Data Configuration
data:
  # Dataset sizes (scaled for 1B model)
  qa_train_size: 87599  # Full SQuAD train set
  qa_val_size: 10570    # Full SQuAD validation
  summary_train_size: 287113  # Full CNN/DailyMail train
  summary_val_size: 13368     # Full CNN/DailyMail validation
  
  # Data Processing
  cache_dir: "./data_cache"
  preprocessing_num_workers: 8
  max_train_samples: -1  # Use all available
  max_eval_samples: 5000  # Limit for faster evaluation
  
  # Streaming (for very large datasets)
  use_streaming: false
  streaming_batch_size: 1000

# Model Architecture (1B Parameter Target)
model_architecture:
  target_parameters: 1000000000
  
  # T5 1B Configuration
  vocab_size: 32128
  d_model: 1024      # Hidden dimension
  d_kv: 64           # Key-value dimension
  d_ff: 4096         # Feed-forward dimension
  num_layers: 24     # Encoder layers
  num_decoder_layers: 24  # Decoder layers
  num_heads: 16      # Attention heads
  dropout_rate: 0.1
  
  # Advanced Features
  relative_attention: true
  use_cache: true
  gradient_checkpointing: true

# System Resources
resources:
  # Hardware Requirements
  min_gpu_memory_gb: 24    # RTX 3090/4090 minimum
  recommended_gpu_memory_gb: 40  # A100 recommended
  cpu_cores: 16
  max_cpu_memory_gb: 64
  
  # Multi-GPU Configuration
  enable_model_parallel: false
  enable_data_parallel: true
  world_size: 1  # Number of GPUs
  
  # Memory Management
  empty_cache_steps: 100
  max_memory_usage: 0.9  # 90% of available GPU memory

# Paths
paths:
  output_dir: "./outputs"
  model_dir: "./models"
  logs_dir: "./logs"
  cache_dir: "./cache"
  tensorboard_log_dir: "./runs"
  wandb_project: "reuko-1b"

# Monitoring & Logging
monitoring:
  use_wandb: true
  use_tensorboard: true
  log_model_architecture: true
  log_gradients: false  # Can be expensive
  log_parameters: true
  save_model_plots: true
  
  # Evaluation Metrics
  compute_metrics: true
  metrics: ["bleu", "rouge", "exact_match", "f1"]

# Task-specific Configuration
tasks:
  qa:
    task_prefix: "<qa>"
    max_context_length: 800
    max_question_length: 100
    max_answer_length: 50
    
  summarization:
    task_prefix: "<summary>"
    max_article_length: 800
    max_summary_length: 150
    min_summary_length: 20
    
  # Multi-task Learning
  multi_task:
    enabled: true
    task_sampling_strategy: "proportional"  # or "temperature"
    task_weights:
      qa: 1.0
      summarization: 1.0
