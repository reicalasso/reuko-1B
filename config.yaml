# Reuko-1B Configuration - Flawless 1B+ Parameter Training

model:
  # Model Selection (pick one):
  name: "t5-large"  # 770M params - closest to 1B
  # name: "google/flan-t5-large"  # 780M params - instruction tuned
  # name: "microsoft/DialoGPT-large"  # Alternative for conversational tasks
  
  # Architecture
  use_custom_1b: true  # Use our custom 1B architecture
  max_input_length: 1024  # Modern context length
  max_output_length: 256  # Detailed outputs
  
  # Generation Parameters
  generation_params:
    num_beams: 5
    temperature: 0.8
    top_p: 0.95
    repetition_penalty: 1.2
    length_penalty: 1.5

# Modern Training Configuration
training:
  # Optimizer & Learning Rate
  optimizer: "adamw_torch_fused"  # Fused AdamW for performance
  learning_rate: 1e-4  # Conservative for large models
  weight_decay: 0.1  # Strong regularization
  
  # Training Schedule
  num_epochs: 3
  max_steps: -1  # Set to specific number if needed
  batch_size: 1  # Per device batch size
  gradient_accumulation_steps: 32  # Effective batch = 32
  
  # Learning Rate Schedule
  lr_scheduler_type: "cosine"  # Modern choice
  warmup_ratio: 0.05  # 5% of training for warmup
  warmup_steps: 1000  # Alternative to ratio
  
  # Memory Optimizations
  bf16: true   # Better than FP16 for modern hardware
  fp16_opt_level: "O1"
  dataloader_pin_memory: true
  dataloader_num_workers: 4
  
  # Checkpointing
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 2
  eval_strategy: "steps"
  eval_steps: 500
  logging_steps: 10
  
  # Advanced Optimizations
  gradient_checkpointing: true  # Critical for 1B models

# Multi-task Data Configuration
data:
  # Dataset sizes (scaled for 1B model)
  qa_train_size: 87599  # Full SQuAD train set
  qa_val_size: 10570    # Full SQuAD validation
  summary_train_size: 287113  # Full CNN/DailyMail train
  summary_val_size: 13368     # Full CNN/DailyMail validation
  
  # Data Processing
  preprocessing_num_workers: 8
  max_train_samples: -1  # Use all available
  max_eval_samples: 5000  # Limit for faster evaluation
  
  # Streaming (for very large datasets)
  use_streaming: false # Set to true for massive datasets

# Model Architecture (1B Parameter Target)
model_architecture:
  target_parameters: 1000000000
  
  # T5 1B Configuration
  vocab_size: 32128
  d_model: 1024      # Hidden dimension
  d_kv: 64           # Key-value dimension
  d_ff: 4096         # Feed-forward dimension
  num_layers: 24     # Encoder layers
  num_decoder_layers: 24  # Decoder layers
  num_heads: 16      # Attention heads
  dropout_rate: 0.1
  
  # Advanced Features
  relative_attention: true
  use_cache: true
  gradient_checkpointing: true

# System Resources
resources:
  # Hardware Requirements
  min_gpu_memory_gb: 24    # RTX 3090/4090 minimum
  recommended_gpu_memory_gb: 40  # A100 recommended
  cpu_cores: 16
  max_cpu_memory_gb: 64
  
  # Multi-GPU Configuration
  enable_model_parallel: false
  enable_data_parallel: true
  world_size: 1  # Number of GPUs
  
  # Memory Management
  empty_cache_steps: 100
  max_memory_usage: 0.9  # 90% of available GPU memory

# Paths
paths:
  output_dir: "./outputs"
  model_dir: "./models"
  logs_dir: "./logs"
  cache_dir: "./cache"

# --- Advanced Training Configurations ---

# DeepSpeed for Large Model Training
deepspeed:
  enabled: true
  config_path: "ds_config.json" # Path to DeepSpeed config

# Parameter-Efficient Fine-Tuning (PEFT) with LoRA
peft:
  enabled: true
  lora_r: 16  # LoRA rank (attention dimension)
  lora_alpha: 32  # LoRA alpha (scaling factor)
  lora_dropout: 0.05
  target_modules: ["q", "v"] # Target attention projections for LoRA

# Monitoring & Logging
monitoring:
  use_wandb: true
  wandb_project: "reuko-1b-finetuning"
  log_gradients: false

# Multi-task Learning
multi_task:
  enabled: true
  task_sampling_strategy: "proportional"
  task_weights:
    qa: 1.0
    summarization: 1.0
# Task-specific Configuration
tasks:
  qa:
    task_prefix: "<qa>"
    max_context_length: 800
    max_question_length: 100
    max_answer_length: 50
    
  summarization:
    task_prefix: "<summary>"
    max_article_length: 800
    max_summary_length: 150
    min_summary_length: 20
    
  # Multi-task Learning
  multi_task:
    enabled: true
    task_sampling_strategy: "proportional"  # or "temperature"
    task_weights:
      qa: 1.0
      summarization: 1.0
